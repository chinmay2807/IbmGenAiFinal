{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dialogue Summarization and Detoxification with FLAN-T5, PPO, and Open-Source Tools"
      ],
      "metadata": {
        "id": "wXmexbsaSwsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Notebook Overview\n",
        "This notebook demonstrates how to fine-tune a FLAN-T5 model for dialogue summarization while reducing toxicity in the generated summaries. It uses Proximal Policy Optimization (PPO) for reinforcement learning, with a reward model that encourages non-toxic outputs.\n",
        "All steps use only open-source models and libraries, and the notebook is fully compatible with Google Colab (GPU)."
      ],
      "metadata": {
        "id": "F37JQxzDS2r9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Dependencies"
      ],
      "metadata": {
        "id": "tvUa9AgtS9Sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2\n",
        "!pip install numpy==1.26.4 --force-reinstall\n",
        "!pip install transformers==4.41.0 datasets==2.19.1 peft==0.11.1 trl==0.8.6 evaluate==0.4.2 rouge_score==0.1.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fI0xgU8KK0AD",
        "outputId": "09dd2198-4f95-4b96-e797-2deb1394740d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: torchvision==0.17.2 in /usr/local/lib/python3.11/dist-packages (0.17.2)\n",
            "Requirement already satisfied: torchaudio==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (4.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2024.3.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.17.2) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.17.2) (11.2.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.2) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.2) (1.3.0)\n",
            "Collecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "8fa81773ab7940459bf40374856ae84d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.41.0 in /usr/local/lib/python3.11/dist-packages (4.41.0)\n",
            "Requirement already satisfied: datasets==2.19.1 in /usr/local/lib/python3.11/dist-packages (2.19.1)\n",
            "Requirement already satisfied: peft==0.11.1 in /usr/local/lib/python3.11/dist-packages (0.11.1)\n",
            "Requirement already satisfied: trl==0.8.6 in /usr/local/lib/python3.11/dist-packages (0.8.6)\n",
            "Requirement already satisfied: evaluate==0.4.2 in /usr/local/lib/python3.11/dist-packages (0.4.2)\n",
            "Requirement already satisfied: rouge_score==0.1.2 in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (0.33.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (18.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (0.7)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets==2.19.1) (2024.3.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (3.11.15)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.11.1) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.11.1) (2.2.2)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.11.1) (1.8.1)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.11/dist-packages (from trl==0.8.6) (0.9.26)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score==0.1.2) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score==0.1.2) (3.9.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score==0.1.2) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.0) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.0) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (2025.6.15)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft==0.11.1) (12.5.82)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.8.6) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.8.6) (13.9.4)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.8.6) (1.7.2)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.8.6) (4.4.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score==0.1.2) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score==0.1.2) (1.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.19.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.19.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.19.1) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (2.19.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft==0.11.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.13.0->peft==0.11.1) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Libraries"
      ],
      "metadata": {
        "id": "IJuZ8T0BTC34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead, create_reference_model\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "KsvBxh21K0fU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load and Prepare Dataset\n",
        "\n",
        "\n",
        "*   Loads the open-source DialogSum dataset for dialogue summarization.\n",
        "*   Filters out very short or long dialogues.\n",
        "*   Tokenizes each dialogue into a prompt for the model.\n",
        "*   Splits the data into training and test sets.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sLmVjqM6THPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"knkarthick/dialogsum\")\n",
        "\n",
        "def build_dataset(tokenizer, min_len=200, max_len=1000):\n",
        "    ds = dataset[\"train\"].filter(lambda x: min_len < len(x[\"dialogue\"]) <= max_len)\n",
        "    def tokenize(sample):\n",
        "        prompt = f\"Summarize the following conversation.\\n\\n{sample['dialogue']}\\n\\nSummary:\\n\"\n",
        "        sample[\"input_ids\"] = tokenizer.encode(prompt, truncation=True, max_length=512)\n",
        "        sample[\"query\"] = prompt\n",
        "        return sample\n",
        "    ds = ds.map(tokenize)\n",
        "    ds.set_format(type=\"torch\")\n",
        "    return ds.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "dataset_splits = build_dataset(tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkRsymqYLjbL",
        "outputId": "81fbc2bd-d2d5-4295-da55-54e3d2d7bce4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load FLAN-T5 Model and Add LoRA Adapter\n",
        "\n",
        "*   Sets up LoRA (Low-Rank Adaptation) configuration for parameter-efficient fine-tuning.\n",
        "*   Loads the FLAN-T5 base model.\n",
        "*   Wraps the model with LoRA for efficient RL fine-tuning.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jBzjlXnDTK-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM\n",
        ")\n",
        "\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.float32)\n",
        "peft_model = get_peft_model(base_model, lora_config)\n"
      ],
      "metadata": {
        "id": "XI26OGavLo1U"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Toxicity Reward Model\n",
        "\n",
        "*   Loads a RoBERTa-based model for detecting hate speech.\n",
        "*   Defines a function to score outputs: higher reward for less toxic (non-hate) content.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q8xhWZvUTQ4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toxicity_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
        "toxicity_tokenizer = AutoTokenizer.from_pretrained(toxicity_model_name)\n",
        "toxicity_model = AutoModelForSequenceClassification.from_pretrained(toxicity_model_name)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "toxicity_model = toxicity_model.to(device)\n",
        "\n",
        "def get_nothate_reward(texts):\n",
        "    inputs = toxicity_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = toxicity_model(**inputs).logits\n",
        "    rewards = logits[:, 0].cpu().tolist()  # logit for \"nothate\"\n",
        "    return rewards\n"
      ],
      "metadata": {
        "id": "EXEF3-AcLuZS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prepare PPO Model and Reference Model\n",
        "\n",
        "\n",
        "*   Wraps the LoRA model with a value head for PPO.\n",
        "*   Creates a reference model for PPO training stability.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ub9JY7wzT-pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(peft_model)\n",
        "ref_model = create_reference_model(ppo_model)\n"
      ],
      "metadata": {
        "id": "0svCWlNmL00_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Set Up PPO Training\n",
        "\n",
        "\n",
        "*   Configures PPO hyperparameters.\n",
        "*   Defines a data collator for batching.\n",
        "*   Initializes the PPO trainer with all components."
      ],
      "metadata": {
        "id": "mfx9syvvUGG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ppo_config = PPOConfig(\n",
        "    model_name=model_name,\n",
        "    learning_rate=1.41e-5,\n",
        "    ppo_epochs=1,\n",
        "    mini_batch_size=4,\n",
        "    batch_size=8\n",
        ")\n",
        "\n",
        "def collator(data):\n",
        "    return {key: [d[key] for d in data] for key in data[0]}\n",
        "\n",
        "ppo_trainer = PPOTrainer(\n",
        "    config=ppo_config,\n",
        "    model=ppo_model,\n",
        "    ref_model=ref_model,\n",
        "    tokenizer=tokenizer,\n",
        "    dataset=dataset_splits[\"train\"],\n",
        "    data_collator=collator\n",
        ")\n"
      ],
      "metadata": {
        "id": "5l0pNgDpMNSn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PPO Training Loop\n",
        "*   Runs PPO for 10 batches (for demonstration).\n",
        "*   Generates summaries for each prompt.\n",
        "*   Scores each summary for non-toxicity.\n",
        "*   Updates the model with PPO using the rewards.\n",
        "*   Logs training statistics."
      ],
      "metadata": {
        "id": "FlYlqCw0URAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_min_length, output_max_length = 30, 100\n",
        "\n",
        "for step, batch in tqdm(enumerate(ppo_trainer.dataloader), total=10):\n",
        "    if step >= 10: break\n",
        "    prompt_tensors = batch[\"input_ids\"]\n",
        "    responses = []\n",
        "    responses = ppo_trainer.generate(\n",
        "      [p for p in prompt_tensors],\n",
        "      max_new_tokens=output_max_length,\n",
        "      do_sample=True\n",
        "    )\n",
        "    batch[\"response\"] = [tokenizer.decode(r, skip_special_tokens=True) for r in responses]\n",
        "    query_response = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
        "    rewards = get_nothate_reward(query_response)\n",
        "    reward_tensors = [torch.tensor(r) for r in rewards]\n",
        "    stats = ppo_trainer.step(prompt_tensors, responses, reward_tensors)\n",
        "    ppo_trainer.log_stats(stats, batch, reward_tensors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ro3TkjvTMPXi",
        "outputId": "41f80581-c0f7-4dbe-abe0-178bd172ce78"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100%|██████████| 10/10 [00:41<00:00,  4.17s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluate Toxicity of Model Outputs\n",
        "* Evaluates the trained model's outputs on the test set.\n",
        "\n",
        "* Computes the mean and standard deviation of toxicity scores for the generated summaries.\n",
        "\n",
        "* Prints the results, showing how well the model avoids toxic content.\n",
        "\n"
      ],
      "metadata": {
        "id": "TEJV0Z9tUf_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_toxicity(model, tokenizer, dataset, n=10):\n",
        "    toxicities = []\n",
        "    for i, sample in enumerate(dataset):\n",
        "        if i >= n: break\n",
        "        input_ids = tokenizer(sample[\"query\"], return_tensors=\"pt\").input_ids.to(device)\n",
        "        with torch.no_grad():\n",
        "            summary_ids = model.generate(input_ids=input_ids, max_new_tokens=output_max_length)\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        # Use the reward function for quick toxicity check\n",
        "        tox = get_nothate_reward([sample[\"query\"] + summary])[0]\n",
        "        toxicities.append(tox)\n",
        "    return np.mean(toxicities), np.std(toxicities)\n",
        "\n",
        "mean, std = evaluate_toxicity(ppo_model, tokenizer, dataset_splits[\"test\"])\n",
        "print(f\"Toxicity after PPO: mean={mean:.4f}, std={std:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKNj3OgHMRTr",
        "outputId": "4b40e695-9093-4011-c9d6-e61cc29b93ea"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Toxicity after PPO: mean=3.0666, std=0.5770\n"
          ]
        }
      ]
    }
  ]
}